# FastSpeech2 å®Œæ•´å®ç°

åŸºäº HuggingFace LJSpeech æ•°æ®é›†çš„ FastSpeech2 æ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿã€‚

## ğŸ“‹ ç›®å½•ç»“æ„

```
.
â”œâ”€â”€ fastspeech2_improved.py          # æ”¹è¿›çš„æ¨¡å‹å®ç°ï¼ˆå«å®Œæ•´Maskæ”¯æŒï¼‰
â”œâ”€â”€ fastspeech2_real_data.py         # çœŸå®æ•°æ®åŠ è½½ï¼ˆHuggingFace LJSpeechï¼‰
â”œâ”€â”€ train_fastspeech2_complete.py    # å®Œæ•´è®­ç»ƒè„šæœ¬
â”œâ”€â”€ inference_fastspeech2_real.py    # æ¨ç†è„šæœ¬
â”œâ”€â”€ processed_data/                  # é¢„å¤„ç†åçš„æ•°æ®
â”‚   â”œâ”€â”€ tokenizer.json              # éŸ³ç´ è¯è¡¨
â”‚   â”œâ”€â”€ stats.json                  # ç»Ÿè®¡ä¿¡æ¯
â”‚   â”œâ”€â”€ train_samples.pkl           # è®­ç»ƒæ ·æœ¬
â”‚   â””â”€â”€ validation_samples.pkl      # éªŒè¯æ ·æœ¬
â”œâ”€â”€ checkpoints/                     # æ¨¡å‹æ£€æŸ¥ç‚¹
â””â”€â”€ outputs/                         # åˆæˆè¾“å‡º
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install torch torchaudio
pip install datasets transformers
pip install librosa phonemizer
pip install scipy matplotlib tqdm
```

**æ³¨æ„**: `phonemizer` éœ€è¦ `espeak` æˆ– `espeak-ng`:

```bash
# Ubuntu/Debian
sudo apt-get install espeak-ng

# macOS
brew install espeak

# Windows
# ä¸‹è½½å¹¶å®‰è£…: https://github.com/espeak-ng/espeak-ng/releases
```

### 2. æ•°æ®é¢„å¤„ç†

é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½ LJSpeech æ•°æ®é›†å¹¶é¢„å¤„ç†ï¼š

```bash
python fastspeech2_real_data.py
```

è¿™å°†ï¼š
- ä» HuggingFace ä¸‹è½½ LJSpeech æ•°æ®é›†
- è¿›è¡Œæ–‡æœ¬åˆ°éŸ³ç´ çš„è½¬æ¢ï¼ˆG2Pï¼‰
- æå–æ¢…å°”é¢‘è°±ã€pitchã€energy
- ä¼°ç®— durationï¼ˆç®€åŒ–æ–¹æ³•ï¼‰
- ä¿å­˜é¢„å¤„ç†æ•°æ®

**é¢„å¤„ç†æ—¶é—´**: é¦–æ¬¡çº¦ 30-60 åˆ†é’Ÿï¼ˆå–å†³äºç½‘ç»œé€Ÿåº¦å’Œè®¡ç®—èµ„æºï¼‰

### 3. è®­ç»ƒæ¨¡å‹

#### å¿«é€Ÿæµ‹è¯•ï¼ˆ100ä¸ªæ ·æœ¬ï¼‰

```bash
python train_fastspeech2_complete.py \
    --max_train_samples 100 \
    --max_val_samples 20 \
    --num_epochs 50 \
    --batch_size 8
```

#### å®Œæ•´è®­ç»ƒ

```bash
python train_fastspeech2_complete.py \
    --num_epochs 100 \
    --batch_size 16 \
    --learning_rate 1e-4 \
    --d_model 256 \
    --n_layers 4 \
    --n_heads 2
```

#### è®­ç»ƒå‚æ•°è¯´æ˜

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--num_epochs` | 100 | è®­ç»ƒè½®æ•° |
| `--batch_size` | 16 | æ‰¹å¤§å° |
| `--learning_rate` | 1e-4 | å­¦ä¹ ç‡ |
| `--d_model` | 256 | æ¨¡å‹éšè—å±‚ç»´åº¦ |
| `--n_layers` | 4 | Transformerå±‚æ•° |
| `--n_heads` | 2 | æ³¨æ„åŠ›å¤´æ•° |
| `--d_ff` | 1024 | å‰é¦ˆç½‘ç»œç»´åº¦ |
| `--checkpoint_dir` | ./checkpoints | æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½• |
| `--resume` | None | ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ |

### 4. è¯­éŸ³åˆæˆï¼ˆæ¨ç†ï¼‰

#### äº¤äº’å¼æ¨¡å¼

```bash
python inference_fastspeech2_real.py
```

ç„¶åè¾“å…¥æ–‡æœ¬è¿›è¡Œåˆæˆï¼š

```
>> Hello world, this is a test.
>> speed 0.8    # åŠ å¿«è¯­é€Ÿ
>> pitch 1.2    # æé«˜éŸ³é«˜
>> Hello again with different voice.
>> quit
```

#### å‘½ä»¤è¡Œæ¨¡å¼

å•ä¸ªæ–‡æœ¬ï¼š

```bash
python inference_fastspeech2_real.py \
    --checkpoint checkpoints/best_model.pth \
    --text "Hello world, this is a test." \
    --output_dir outputs \
    --save_mel
```

æ‰¹é‡åˆæˆï¼ˆä»æ–‡ä»¶ï¼‰ï¼š

```bash
python inference_fastspeech2_real.py \
    --checkpoint checkpoints/best_model.pth \
    --text_file texts.txt \
    --output_dir outputs
```

#### æ§åˆ¶å‚æ•°

```bash
# å¿«é€Ÿè¯­é€Ÿ
python inference_fastspeech2_real.py \
    --checkpoint checkpoints/best_model.pth \
    --text "Fast speech" \
    --duration_control 0.8

# é«˜éŸ³è°ƒ
python inference_fastspeech2_real.py \
    --checkpoint checkpoints/best_model.pth \
    --text "High pitch" \
    --pitch_control 1.3

# ä½èƒ½é‡ï¼ˆè½»å£°ï¼‰
python inference_fastspeech2_real.py \
    --checkpoint checkpoints/best_model.pth \
    --text "Soft voice" \
    --energy_control 0.7
```

## ğŸ“Š æ¨¡å‹æ¶æ„

### æ ¸å¿ƒç»„ä»¶

1. **ç¼–ç å™¨** (Encoder)
   - å¤šå±‚ Feed-Forward Transformer
   - å¤„ç†éŸ³ç´ çº§åˆ«çš„æ–‡æœ¬è¡¨å¾

2. **æ–¹å·®é€‚é…å™¨** (Variance Adaptor)
   - Duration Predictor: é¢„æµ‹æ¯ä¸ªéŸ³ç´ çš„æŒç»­æ—¶é•¿
   - Pitch Predictor: é¢„æµ‹éŸ³é«˜
   - Energy Predictor: é¢„æµ‹èƒ½é‡
   - Length Regulator: æ ¹æ® duration æ‰©å±•åºåˆ—

3. **è§£ç å™¨** (Decoder)
   - å¤šå±‚ Feed-Forward Transformer
   - ç”Ÿæˆå¸§çº§åˆ«çš„æ¢…å°”é¢‘è°±

### å…³é”®ç‰¹æ€§

âœ… **å®Œæ•´çš„ Mask æ”¯æŒ**
- Text Mask: å¤„ç†å˜é•¿æ–‡æœ¬è¾“å…¥
- Mel Mask: å¤„ç†å˜é•¿æ¢…å°”é¢‘è°±è¾“å‡º
- æ­£ç¡®çš„æŸå¤±è®¡ç®—ï¼ˆåªåœ¨æœ‰æ•ˆä½ç½®ï¼‰

âœ… **ç²¾ç¡®çš„é•¿åº¦å¯¹é½**
- Duration ä¸æ¢…å°”é¢‘è°±å¸§æ•°å¯¹é½
- è®­ç»ƒæ—¶ä½¿ç”¨çœŸå® duration
- æ¨ç†æ—¶ä½¿ç”¨é¢„æµ‹ duration

âœ… **å¯¹æ•°åŸŸ Duration é¢„æµ‹**
- è®­ç»ƒ: `log(duration + 1)`
- æ¨ç†: `exp(pred) - 1`
- æ•°å€¼ç¨³å®šï¼Œæ”¶æ•›æ›´å¿«

## ğŸ“ˆ è®­ç»ƒç›‘æ§

è®­ç»ƒæ—¥å¿—ä¿å­˜åœ¨ `logs/training_log.jsonl`ï¼š

```python
import json
import matplotlib.pyplot as plt

# è¯»å–æ—¥å¿—
logs = []
with open('logs/training_log.jsonl', 'r') as f:
    for line in f:
        logs.append(json.loads(line))

# ç»˜åˆ¶æŸå¤±æ›²çº¿
epochs = [log['epoch'] for log in logs]
train_loss = [log['train_loss'] for log in logs]
val_loss = [log['val_loss'] for log in logs]

plt.plot(epochs, train_loss, label='Train Loss')
plt.plot(epochs, val_loss, label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.savefig('training_curve.png')
```

## ğŸ”§ å¸¸è§é—®é¢˜

### 1. Duration å¯¹é½é—®é¢˜

**Q: å¦‚ä½•ä¿è¯ LengthRegulator è¾“å‡ºé•¿åº¦ä¸æ¢…å°”é¢‘è°±ä¸€è‡´ï¼Ÿ**

A: é€šè¿‡çœŸå®çš„ duration æ ‡æ³¨ä¿è¯ï¼š
- æ•°æ®é¢„å¤„ç†æ—¶ä½¿ç”¨å¼ºåˆ¶å¯¹é½è·å– duration
- `sum(duration)` å¤©ç„¶ç­‰äºæ¢…å°”é¢‘è°±å¸§æ•°
- è®­ç»ƒæ—¶ä½¿ç”¨çœŸå® durationï¼Œä¿è¯å¯¹é½

**å½“å‰å®ç°**: ä½¿ç”¨ç®€åŒ–çš„å¹³å‡åˆ†é…ä¼°ç®— durationã€‚å®é™…é¡¹ç›®ä¸­åº”ä½¿ç”¨ **Montreal Forced Aligner (MFA)** è¿›è¡Œç²¾ç¡®å¯¹é½ã€‚

### 2. ä½¿ç”¨ MFA è¿›è¡Œç²¾ç¡®å¯¹é½

```bash
# 1. å®‰è£… MFA
conda install -c conda-forge montreal-forced-aligner

# 2. ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹
mfa model download acoustic english_us_arpa
mfa model download dictionary english_us_arpa

# 3. å‡†å¤‡æ•°æ®
# éŸ³é¢‘: corpus/speaker/audio.wav
# æ–‡æœ¬: corpus/speaker/audio.txt

# 4. æ‰§è¡Œå¯¹é½
mfa align corpus/ english_us_arpa english_us_arpa aligned_output/

# 5. æå– duration
# è§£æ TextGrid æ–‡ä»¶è·å–æ¯ä¸ªéŸ³ç´ çš„æ—¶é—´æˆ³
```

### 3. ä¸ºä»€ä¹ˆéœ€è¦ Maskï¼Ÿ

**å¿…é¡»ä½¿ç”¨ Mask çš„åŸå› **:
1. Batch ä¸­åºåˆ—é•¿åº¦ä¸åŒï¼Œéœ€è¦ padding
2. Padding ä½ç½®ä¸åº”å‚ä¸æ³¨æ„åŠ›è®¡ç®—
3. æŸå¤±è®¡ç®—è¦å‡†ç¡®ï¼ˆåªè®¡ç®—æœ‰æ•ˆä½ç½®ï¼‰
4. ä¸ç”¨ Mask ä¼šå­¦åˆ°è™šå‡çš„ padding æ¨¡å¼

### 4. æ”¹è¿›å£°ç å™¨

å½“å‰ä½¿ç”¨ Griffin-Lim ç®—æ³•ï¼ŒéŸ³è´¨ä¸€èˆ¬ã€‚å»ºè®®ä½¿ç”¨ç¥ç»å£°ç å™¨ï¼š

**æ¨èé€‰æ‹©**:
- **HiFi-GAN**: å¿«é€Ÿï¼ŒéŸ³è´¨å¥½
- **WaveGlow**: éŸ³è´¨ä¼˜ç§€
- **Parallel WaveGAN**: å¹³è¡¡é€Ÿåº¦å’Œè´¨é‡

```python
# ä½¿ç”¨ HiFi-GAN (ç¤ºä¾‹)
from vocoder import HiFiGAN

vocoder = HiFiGAN(checkpoint='hifigan_checkpoint.pth')
audio = vocoder.mel_to_audio(mel_spectrogram)
```

## ğŸ“ æ€§èƒ½ä¼˜åŒ–

### è®­ç»ƒåŠ é€Ÿ

1. **ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ**:
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    output = model(input)
    loss = criterion(output, target)
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

2. **å¢åŠ  batch size** (å¦‚æœæ˜¾å­˜å…è®¸)

3. **ä½¿ç”¨å¤š GPU**:
```python
model = nn.DataParallel(model)
```

### æ¨ç†åŠ é€Ÿ

1. **é‡åŒ–æ¨¡å‹**:
```python
model_int8 = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)
```

2. **ä½¿ç”¨ ONNX**:
```python
torch.onnx.export(model, dummy_input, "fastspeech2.onnx")
```

## ğŸ¯ ä¸‹ä¸€æ­¥æ”¹è¿›

### çŸ­æœŸç›®æ ‡

- [ ] ä½¿ç”¨ MFA è¿›è¡Œç²¾ç¡®çš„å¼ºåˆ¶å¯¹é½
- [ ] é›†æˆ HiFi-GAN å£°ç å™¨
- [ ] æ·»åŠ è¯´è¯äºº embeddingï¼ˆå¤šè¯´è¯äººæ”¯æŒï¼‰
- [ ] å®ç° Conformer æ›¿ä»£ Transformer

### é•¿æœŸç›®æ ‡

- [ ] æ”¯æŒä¸­æ–‡ TTS
- [ ] æ·»åŠ æƒ…æ„Ÿæ§åˆ¶
- [ ] å®æ—¶æµå¼åˆæˆ
- [ ] Web ç•Œé¢

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **FastSpeech 2**: Ren, Y., et al. (2020). "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech"
2. **LJSpeech**: Keith Ito and Linda Johnson. "The LJ Speech Dataset"
3. **Transformer**: Vaswani, A., et al. (2017). "Attention is All You Need"

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®ä»…ç”¨äºå­¦ä¹ å’Œç ”ç©¶ç›®çš„ã€‚

## ğŸ™ è‡´è°¢

- HuggingFace æä¾›çš„ LJSpeech æ•°æ®é›†
- FastSpeech2 åŸä½œè€…
- å¼€æºç¤¾åŒºçš„è´¡çŒ®

---

**ä½œè€…**: FastSpeech2 å®ç°å›¢é˜Ÿ  
**æ›´æ–°æ—¥æœŸ**: 2025-10-16
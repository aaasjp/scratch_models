from tkinter import E
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import json
import os
import matplotlib.pyplot as plt
import matplotlib
from torchviz import make_dot
import tempfile

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ"""
    import platform
    
    system = platform.system()
    chinese_fonts = []
    
    if system == "Darwin":  # macOS
        chinese_fonts = ['Arial Unicode MS', 'PingFang SC', 'Hiragino Sans GB', 'STHeiti']
    elif system == "Windows":
        chinese_fonts = ['SimHei', 'Microsoft YaHei', 'KaiTi', 'FangSong']
    else:  # Linux
        chinese_fonts = ['WenQuanYi Micro Hei', 'WenQuanYi Zen Hei', 'Noto Sans CJK SC']
    
    # æ·»åŠ é€šç”¨å­—ä½“ä½œä¸ºå¤‡é€‰
    chinese_fonts.extend(['DejaVu Sans', 'Liberation Sans', 'Arial'])
    
    plt.rcParams['font.sans-serif'] = chinese_fonts
    plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
    
    # æµ‹è¯•å­—ä½“æ˜¯å¦å¯ç”¨
    try:
        fig, ax = plt.subplots(figsize=(1, 1))
        ax.text(0.5, 0.5, 'æµ‹è¯•ä¸­æ–‡', fontsize=12)
        plt.close(fig)
        print(f"âœ… ä¸­æ–‡å­—ä½“è®¾ç½®æˆåŠŸï¼Œä½¿ç”¨å­—ä½“: {plt.rcParams['font.sans-serif'][0]}")
    except Exception as e:
        print(f"âš ï¸ ä¸­æ–‡å­—ä½“è®¾ç½®å¯èƒ½æœ‰é—®é¢˜: {e}")
        print("å»ºè®®å®‰è£…ä¸­æ–‡å­—ä½“æˆ–ä½¿ç”¨è‹±æ–‡æ ‡ç­¾")

# åˆå§‹åŒ–ä¸­æ–‡å­—ä½“
# setup_chinese_font()


def get_mask_from_lengths(lengths, max_len=None):
    """
    æ ¹æ®åºåˆ—é•¿åº¦ç”Ÿæˆ padding mask
    
    Args:
        lengths: [batch] - æ¯ä¸ªåºåˆ—çš„å®é™…é•¿åº¦
        max_len: æœ€å¤§é•¿åº¦ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        mask: [batch, max_len] - True è¡¨ç¤ºæœ‰æ•ˆä½ç½®ï¼ŒFalse è¡¨ç¤º padding
    """
    batch_size = lengths.shape[0]
    if max_len is None:
        max_len = torch.max(lengths).item()
    
    ids = torch.arange(0, max_len, device=lengths.device).unsqueeze(0).expand(batch_size, -1)
    mask = ids < lengths.unsqueeze(1)
    
    return mask


def get_attn_mask_from_padding_mask(padding_mask):
    """
    ä» padding mask ç”Ÿæˆ attention mask
    
    Args:
        padding_mask: [batch, seq_len] - True è¡¨ç¤ºæœ‰æ•ˆï¼ŒFalse è¡¨ç¤º padding
    
    Returns:
        attn_mask: [batch, 1, seq_len, seq_len] - ç”¨äºæ³¨æ„åŠ›æœºåˆ¶
    """
    batch_size, seq_len = padding_mask.shape
    
    # [batch, seq_len, 1] * [batch, 1, seq_len] -> [batch, seq_len, seq_len]
    attn_mask = padding_mask.unsqueeze(-1) * padding_mask.unsqueeze(1)
    
    # æ·»åŠ  head ç»´åº¦ [batch, 1, seq_len, seq_len]
    attn_mask = attn_mask.unsqueeze(1)
    
    return attn_mask


class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆæ”¯æŒ Maskï¼‰"""
    def __init__(self, d_model, n_heads, dropout=0.1):
        super().__init__()
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.fc = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, query, key, value, mask=None):
        batch_size = query.shape[0]
        
        # Linear projection and split into heads
        Q = self.w_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = self.w_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.w_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        
        # Scaled dot-product attention
        scale = math.sqrt(self.d_k)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / scale
        
        # Apply mask
        if mask is not None:
            # mask: [batch, 1, seq_len, seq_len]
            scores = scores.masked_fill(~mask, -1e9)
        
        attn = F.softmax(scores, dim=-1)
        attn = self.dropout(attn)
        
        # Apply attention to values
        output = torch.matmul(attn, V)
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        output = self.fc(output)
        
        return output


class PositionwiseFeedforward(nn.Module):
    """ä½ç½®å‰é¦ˆç½‘ç»œ"""
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        return self.fc2(self.dropout(F.relu(self.fc1(x))))


class FFTBlock(nn.Module):
    """Feed-Forward Transformer Blockï¼ˆæ”¯æŒ Maskï¼‰"""
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        self.attention = MultiHeadAttention(d_model, n_heads, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.ff = PositionwiseFeedforward(d_model, d_ff, dropout)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        # Self-attention with residual
        attn_output = self.attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # Feedforward with residual
        ff_output = self.ff(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x


class VariancePredictor(nn.Module):
    """æ–¹å·®é¢„æµ‹å™¨"""
    def __init__(self, d_model, kernel_size=3, dropout=0.5):
        super().__init__()
        self.conv1 = nn.Conv1d(d_model, d_model, kernel_size, padding=kernel_size//2)
        self.norm1 = nn.LayerNorm(d_model)
        self.conv2 = nn.Conv1d(d_model, d_model, kernel_size, padding=kernel_size//2)
        self.norm2 = nn.LayerNorm(d_model)
        self.linear = nn.Linear(d_model, 1)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        """
        Args:
            x: [batch, seq_len, d_model]
            mask: [batch, seq_len] - padding mask
        """
        # Conv layers
        x = x.transpose(1, 2)  # [batch, d_model, seq_len]
        x = self.dropout(F.relu(self.conv1(x)))
        x = x.transpose(1, 2)
        x = self.norm1(x)
        x = x.transpose(1, 2)
        x = self.dropout(F.relu(self.conv2(x)))
        x = x.transpose(1, 2)
        x = self.norm2(x)
        
        # Linear projection
        x = self.linear(x).squeeze(-1)  # [batch, seq_len]
        
        # Apply mask to predictions
        if mask is not None:
            x = x.masked_fill(~mask, 0.0)
        
        return x


class LengthRegulator(nn.Module):
    """é•¿åº¦è°ƒèŠ‚å™¨ï¼ˆæ”¹è¿›ç‰ˆï¼Œå¤„ç†å¯¹é½ï¼‰"""
    def __init__(self):
        super().__init__()
        
    def forward(self, x, duration, max_len=None):
        """
        Args:
            x: [batch, text_len, d_model]
            duration: [batch, text_len] - æ¯ä¸ªéŸ³ç´ çš„å¸§æ•°
            max_len: æœ€å¤§è¾“å‡ºé•¿åº¦ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            output: [batch, mel_len, d_model]
            mel_lengths: [batch] - æ¯ä¸ªæ ·æœ¬çš„å®é™…é•¿åº¦
        """
        batch_size = x.size(0)
        device = x.device
        
        output = []
        mel_lengths = []
        
        for i in range(batch_size):
            expanded = []
            for j in range(x.size(1)):
                # è·å–å½“å‰éŸ³ç´ çš„ durationï¼ˆç¡®ä¿ä¸ºæ•´æ•°ï¼‰
                dur = int(duration[i, j].item())
                
                if dur > 0:
                    # æ‰©å±•è¯¥éŸ³ç´  dur æ¬¡
                    phoneme_repr = x[i, j].unsqueeze(0)  # [1, d_model]
                    expanded.append(phoneme_repr.expand(dur, -1))
            
            if expanded:
                # æ‹¼æ¥æ‰€æœ‰æ‰©å±•çš„éŸ³ç´ 
                seq = torch.cat(expanded, dim=0)
            else:
                # å¦‚æœæ‰€æœ‰ duration éƒ½æ˜¯ 0ï¼Œåˆ›å»ºä¸€ä¸ªç©ºåºåˆ—
                seq = torch.zeros(1, x.size(2), device=device)
            
            output.append(seq)
            mel_lengths.append(seq.size(0))
        
        # Padding åˆ°ç»Ÿä¸€é•¿åº¦
        if max_len is None:
            max_len = max(mel_lengths)
        
        mel_lengths_tensor = torch.LongTensor(mel_lengths).to(device)
        
        output_padded = []
        for seq in output:
            if seq.size(0) < max_len:
                padding = torch.zeros(max_len - seq.size(0), seq.size(1), device=device)
                seq = torch.cat([seq, padding], dim=0)
            elif seq.size(0) > max_len:
                seq = seq[:max_len]  # æˆªæ–­è¿‡é•¿çš„åºåˆ—
            output_padded.append(seq)
        
        output_tensor = torch.stack(output_padded, dim=0)
        
        return output_tensor, mel_lengths_tensor


class VarianceAdaptor(nn.Module):
    """æ–¹å·®é€‚é…å™¨ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
    def __init__(self, d_model, kernel_size=3, dropout=0.5, n_bins=256, stats_path=None):
        super().__init__()
        self.duration_predictor = VariancePredictor(d_model, kernel_size, dropout)
        self.length_regulator = LengthRegulator()
        self.pitch_predictor = VariancePredictor(d_model, kernel_size, dropout)
        self.energy_predictor = VariancePredictor(d_model, kernel_size, dropout)
        
        self.pitch_embedding = nn.Embedding(n_bins, d_model)
        self.energy_embedding = nn.Embedding(n_bins, d_model)
        
        self.n_bins = n_bins
        
        # ä»stats.jsonåŠ è½½ç»Ÿè®¡å€¼ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
        self._load_stats_from_json(stats_path)
    
    def _load_stats_from_json(self, stats_path):
        """ä»stats.jsonæ–‡ä»¶åŠ è½½ç»Ÿè®¡å€¼"""
        if stats_path and os.path.exists(stats_path):
            try:
                with open(stats_path, 'r') as f:
                    stats = json.load(f)
                
                # åŠ è½½pitchç»Ÿè®¡å€¼
                self.register_buffer('pitch_min', torch.tensor(stats.get('pitch_min')))
                self.register_buffer('pitch_max', torch.tensor(stats.get('pitch_max')))
                self.register_buffer('pitch_mean', torch.tensor(stats.get('pitch_mean')))
                self.register_buffer('pitch_std', torch.tensor(stats.get('pitch_std')))
                
                # åŠ è½½energyç»Ÿè®¡å€¼
                self.register_buffer('energy_min', torch.tensor(stats.get('energy_min')))
                self.register_buffer('energy_max', torch.tensor(stats.get('energy_max')))
                self.register_buffer('energy_mean', torch.tensor(stats.get('energy_mean')))
                self.register_buffer('energy_std', torch.tensor(stats.get('energy_std')))
                
                # åŠ è½½durationç»Ÿè®¡å€¼
                self.register_buffer('duration_min', torch.tensor(stats.get('duration_min')))
                self.register_buffer('duration_max', torch.tensor(stats.get('duration_max')))
                self.register_buffer('duration_mean', torch.tensor(stats.get('duration_mean')))
                self.register_buffer('duration_std', torch.tensor(stats.get('duration_std')))
                
                print(f"æˆåŠŸä» {stats_path} åŠ è½½ç»Ÿè®¡å€¼")
                
            except Exception as e:
                print(f"åŠ è½½ç»Ÿè®¡å€¼å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                self._register_default_stats()
        else:
            print("å¤±è´¥ï¼šæœªæ‰¾åˆ°stats.jsonæ–‡ä»¶")
            raise Exception(f"å¤±è´¥ï¼šæœªæ‰¾åˆ°stats.jsonæ–‡ä»¶: {e}") from e
    
    
        
    def get_pitch_embedding(self, pitch, mask=None):
        """å°†è¿ç»­pitchå€¼è½¬æ¢ä¸ºembedding"""
        pitch = torch.clamp(pitch, self.pitch_min, self.pitch_max)
        pitch_bins = ((pitch - self.pitch_min) / (self.pitch_max - self.pitch_min) * (self.n_bins - 1)).long()
        pitch_emb = self.pitch_embedding(pitch_bins)
        
        if mask is not None:
            # å°† padding ä½ç½®çš„ embedding ç½®é›¶
            pitch_emb = pitch_emb.masked_fill(~mask.unsqueeze(-1), 0.0)
        
        return pitch_emb
    
    def get_energy_embedding(self, energy, mask=None):
        """å°†è¿ç»­energyå€¼è½¬æ¢ä¸ºembedding"""
        energy = torch.clamp(energy, self.energy_min, self.energy_max)
        energy_bins = ((energy - self.energy_min) / (self.energy_max - self.energy_min) * (self.n_bins - 1)).long()
        energy_emb = self.energy_embedding(energy_bins)
        
        if mask is not None:
            energy_emb = energy_emb.masked_fill(~mask.unsqueeze(-1), 0.0)
        
        return energy_emb
        
    def forward(self, x, text_mask=None, duration=None, pitch=None, energy=None, 
                max_len=None, duration_control=1.0, pitch_control=1.0, energy_control=1.0,eps=1e-6):
        """
        Args:
            x: [batch, text_len, d_model]
            text_mask: [batch, text_len] - text padding mask
            duration: [batch, text_len] - çœŸå®durationï¼ˆè®­ç»ƒæ—¶ï¼‰
            pitch: [batch, text_len] - çœŸå®pitchï¼ˆè®­ç»ƒæ—¶ï¼‰
            energy: [batch, text_len] - çœŸå®energyï¼ˆè®­ç»ƒæ—¶ï¼‰
            max_len: æœ€å¤§melé•¿åº¦
            duration_control: durationç¼©æ”¾ç³»æ•°ï¼ˆæ¨ç†æ—¶ï¼‰
            pitch_control: pitchç¼©æ”¾ç³»æ•°ï¼ˆæ¨ç†æ—¶ï¼‰
            energy_control: energyç¼©æ”¾ç³»æ•°ï¼ˆæ¨ç†æ—¶ï¼‰
        """
        # é¢„æµ‹ duration, pitch, energy
        duration_pred = self.duration_predictor(x, text_mask)
        pitch_pred = self.pitch_predictor(x, text_mask)
        energy_pred = self.energy_predictor(x, text_mask)
        
        # è®­ç»ƒæ—¶ä½¿ç”¨çœŸå®å€¼ï¼Œæ¨ç†æ—¶ä½¿ç”¨é¢„æµ‹å€¼
        if duration is None:
            # å½’ä¸€åŒ–çš„é€†è¿ç®—ï¼Œå…ˆè¿›è¡Œlogè®¡ç®—ï¼Œå†è¿›è¡Œå½’ä¸€åŒ–
            duration = duration_pred * (float(self.duration_std) + eps) + float(self.duration_mean)
            # åº”ç”¨duration_controlæ§åˆ¶
            duration = duration * duration_control
            # ç¡®ä¿durationä¸ºæ­£å€¼ä¸”ä¸ºæ•´æ•°
            duration = torch.clamp(duration, min=float(self.duration_min), max=float(self.duration_max))
            duration = torch.round(duration)
        
        if pitch is None:
            # å½’ä¸€åŒ–çš„é€†è¿ç®—ï¼Œå…ˆè¿›è¡Œlogè®¡ç®—ï¼Œå†è¿›è¡Œå½’ä¸€åŒ–
            pitch = pitch_pred * (float(self.pitch_std) + eps) + float(self.pitch_mean)
            # åº”ç”¨pitch_controlæ§åˆ¶
            pitch = pitch * pitch_control
            pitch = torch.clamp(pitch, min=float(self.pitch_min), max=float(self.pitch_max))
        
        if energy is None:
            # å½’ä¸€åŒ–çš„é€†è¿ç®—ï¼Œå…ˆè¿›è¡Œlogè®¡ç®—ï¼Œå†è¿›è¡Œå½’ä¸€åŒ–
            energy = energy_pred * (float(self.energy_std) + eps) + float(self.energy_mean)
            # åº”ç”¨energy_controlæ§åˆ¶
            energy = energy * energy_control
            energy = torch.clamp(energy, min=float(self.energy_min), max=float(self.energy_max))
            
        # æ·»åŠ  pitch å’Œ energy embedding
        pitch_emb = self.get_pitch_embedding(pitch, text_mask)
        energy_emb = self.get_energy_embedding(energy, text_mask)
        
        x = x + pitch_emb + energy_emb
        
        # é•¿åº¦è°ƒèŠ‚ï¼ˆå…³é”®ï¼šå°†éŸ³ç´ çº§æ‰©å±•åˆ°å¸§çº§ï¼‰
        x, mel_lengths = self.length_regulator(x, duration, max_len)
        
        # ç”Ÿæˆ mel mask
        mel_mask = get_mask_from_lengths(mel_lengths, x.size(1))
        
        return x, mel_mask, duration_pred, pitch_pred, energy_pred, mel_lengths


class FastSpeech2(nn.Module):
    """FastSpeech2 ä¸»æ¨¡å‹ï¼ˆæ”¹è¿›ç‰ˆï¼Œå®Œæ•´çš„ Mask æ”¯æŒï¼‰"""
    def __init__(
        self,
        vocab_size,
        d_model=256,
        n_layers=8,
        n_heads=2,
        d_ff=1024,
        dropout=0.1,
        n_mel_channels=80,
        max_seq_len=1000,
        stats_path=None
    ):
        super().__init__()
        
        # æ–‡æœ¬ embedding
        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)
        
        # ä½ç½®ç¼–ç 
        self.register_buffer('pos_embedding', self._create_positional_encoding(max_seq_len, d_model))
        
        # ç¼–ç å™¨
        self.encoder = nn.ModuleList([
            FFTBlock(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])
        
        # æ–¹å·®é€‚é…å™¨
        self.variance_adaptor = VarianceAdaptor(d_model, dropout=dropout, stats_path=stats_path)
        
        # è§£ç å™¨
        self.decoder = nn.ModuleList([
            FFTBlock(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])
        
        # è¾“å‡ºå±‚
        self.mel_linear = nn.Linear(d_model, n_mel_channels)
        
        self.d_model = d_model
        
    def _create_positional_encoding(self, max_len, d_model):
        """åˆ›å»ºä½ç½®ç¼–ç """
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        return pe.unsqueeze(0)  # [1, max_len, d_model]
        
    def forward(self, text, text_lengths=None, duration=None, pitch=None, energy=None, 
                max_mel_len=None, duration_control=1.0, pitch_control=1.0, energy_control=1.0):
        """
        Args:
            text: [batch, text_len] - æ–‡æœ¬ç´¢å¼•
            text_lengths: [batch] - æ–‡æœ¬å®é™…é•¿åº¦
            duration: [batch, text_len] - æŒç»­æ—¶é—´ï¼ˆè®­ç»ƒæ—¶æä¾›ï¼‰
            pitch: [batch, text_len] - éŸ³é«˜ï¼ˆè®­ç»ƒæ—¶æä¾›ï¼‰
            energy: [batch, text_len] - èƒ½é‡ï¼ˆè®­ç»ƒæ—¶æä¾›ï¼‰
            max_mel_len: æœ€å¤§melé•¿åº¦ï¼ˆè®­ç»ƒæ—¶æä¾›ï¼‰
            duration_control: è¯­é€Ÿæ§åˆ¶ï¼ˆæ¨ç†æ—¶ï¼‰
            pitch_control: éŸ³é«˜æ§åˆ¶ï¼ˆæ¨ç†æ—¶ï¼‰
            energy_control: èƒ½é‡æ§åˆ¶ï¼ˆæ¨ç†æ—¶ï¼‰
        
        Returns:
            mel_output: [batch, mel_len, n_mel_channels]
            mel_mask: [batch, mel_len]
            duration_pred: [batch, text_len]
            pitch_pred: [batch, text_len]
            energy_pred: [batch, text_len]
            mel_lengths: [batch]
        """
        # ç”Ÿæˆ text mask
        if text_lengths is None:
            text_lengths = torch.full((text.size(0),), text.size(1), 
                                     dtype=torch.long, device=text.device)
        
        text_mask = get_mask_from_lengths(text_lengths, text.size(1))
        text_attn_mask = get_attn_mask_from_padding_mask(text_mask)
        
        # æ–‡æœ¬ embedding + ä½ç½®ç¼–ç 
        x = self.embedding(text) * math.sqrt(self.d_model)
        seq_len = x.size(1)
        x = x + self.pos_embedding[:, :seq_len, :].to(x.device)
        
        # ç¼–ç å™¨
        for layer in self.encoder:
            x = layer(x, text_attn_mask)
        
        # å°† text mask åº”ç”¨åˆ°è¾“å‡ºï¼ˆç¡®ä¿ padding ä½ç½®ä¸º 0ï¼‰
        x = x.masked_fill(~text_mask.unsqueeze(-1), 0.0)
        
        # æ–¹å·®é€‚é…å™¨ï¼ˆåŒ…å« Length Regulatorï¼‰
        x, mel_mask, duration_pred, pitch_pred, energy_pred, mel_lengths = self.variance_adaptor(
            x, text_mask, duration, pitch, energy, max_mel_len,
            duration_control, pitch_control, energy_control
        )
        
        # ç”Ÿæˆ mel attention mask
        mel_attn_mask = get_attn_mask_from_padding_mask(mel_mask)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        mel_len = x.size(1)
        if mel_len <= self.pos_embedding.size(1):
            x = x + self.pos_embedding[:, :mel_len, :].to(x.device)
        else:
            # å¦‚æœåºåˆ—å¤ªé•¿ï¼Œéœ€è¦æ‰©å±•ä½ç½®ç¼–ç 
            new_pe = self._create_positional_encoding(mel_len, self.d_model).to(x.device)
            x = x + new_pe[:, :mel_len, :]
        
        # è§£ç å™¨
        for layer in self.decoder:
            x = layer(x, mel_attn_mask)
        
        # å°† mel mask åº”ç”¨åˆ°è¾“å‡º
        x = x.masked_fill(~mel_mask.unsqueeze(-1), 0.0)
        
        # ç”Ÿæˆæ¢…å°”é¢‘è°±
        mel_output = self.mel_linear(x)
        
        # ç¡®ä¿ padding ä½ç½®çš„ mel ä¸º 0
        mel_output = mel_output.masked_fill(~mel_mask.unsqueeze(-1), 0.0)
        
        return mel_output, mel_mask, duration_pred, pitch_pred, energy_pred, mel_lengths


def print_model_info(model):
    """æ‰“å°æ¨¡å‹è¯¦ç»†ä¿¡æ¯å’Œç»“æ„"""
    print("\n" + "=" * 80)
    print("FastSpeech2 æ¨¡å‹ç»“æ„")
    print("=" * 80)
    print(model)
    
    print("\n" + "=" * 80)
    print("æ¨¡å‹å‚æ•°ç»Ÿè®¡")
    print("=" * 80)
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.2f} MB (FP32)")
    
    print("\n" + "=" * 80)
    print("å„ç»„ä»¶å‚æ•°ç»Ÿè®¡")
    print("=" * 80)
    
    # æ–‡æœ¬åµŒå…¥å±‚
    embedding_params = sum(p.numel() for p in model.embedding.parameters())
    print(f"æ–‡æœ¬åµŒå…¥å±‚å‚æ•°: {embedding_params:,}")
    
    # ç¼–ç å™¨å‚æ•°
    encoder_params = sum(p.numel() for p in model.encoder.parameters())
    print(f"ç¼–ç å™¨å‚æ•°: {encoder_params:,}")
    
    # æ–¹å·®é€‚é…å™¨å‚æ•°
    variance_params = sum(p.numel() for p in model.variance_adaptor.parameters())
    print(f"æ–¹å·®é€‚é…å™¨å‚æ•°: {variance_params:,}")
    
    # è§£ç å™¨å‚æ•°
    decoder_params = sum(p.numel() for p in model.decoder.parameters())
    print(f"è§£ç å™¨å‚æ•°: {decoder_params:,}")
    
    # è¾“å‡ºå±‚å‚æ•°
    output_params = sum(p.numel() for p in model.mel_linear.parameters())
    print(f"è¾“å‡ºå±‚å‚æ•°: {output_params:,}")
    
    print("\n" + "=" * 80)
    print("æ¨¡å‹é…ç½®ä¿¡æ¯")
    print("=" * 80)
    print(f"è¯æ±‡è¡¨å¤§å°: {model.embedding.num_embeddings}")
    print(f"æ¨¡å‹ç»´åº¦: {model.d_model}")
    print(f"ç¼–ç å™¨å±‚æ•°: {len(model.encoder)}")
    print(f"è§£ç å™¨å±‚æ•°: {len(model.decoder)}")
    print(f"æ³¨æ„åŠ›å¤´æ•°: {model.encoder[0].attention.n_heads}")
    print(f"å‰é¦ˆç½‘ç»œç»´åº¦: {model.encoder[0].ff.fc1.out_features}")
    print(f"æ¢…å°”é¢‘è°±é€šé“æ•°: {model.mel_linear.out_features}")
    
    print("\n" + "=" * 80)
    print("æ–¹å·®é€‚é…å™¨è¯¦ç»†ä¿¡æ¯")
    print("=" * 80)
    print(f"Durationé¢„æµ‹å™¨å‚æ•°: {sum(p.numel() for p in model.variance_adaptor.duration_predictor.parameters()):,}")
    print(f"Pitché¢„æµ‹å™¨å‚æ•°: {sum(p.numel() for p in model.variance_adaptor.pitch_predictor.parameters()):,}")
    print(f"Energyé¢„æµ‹å™¨å‚æ•°: {sum(p.numel() for p in model.variance_adaptor.energy_predictor.parameters()):,}")
    print(f"PitchåµŒå…¥ç»´åº¦: {model.variance_adaptor.pitch_embedding.num_embeddings}")
    print(f"EnergyåµŒå…¥ç»´åº¦: {model.variance_adaptor.energy_embedding.num_embeddings}")
    
    print("\n" + "=" * 80)
    print("æ¨¡å‹ç»“æ„å±‚æ¬¡")
    print("=" * 80)
    print("1. æ–‡æœ¬åµŒå…¥å±‚ (Embedding)")
    print("2. ä½ç½®ç¼–ç  (Positional Encoding)")
    print("3. ç¼–ç å™¨ (Encoder)")
    print("   - 8å±‚ FFT Block")
    print("   - æ¯å±‚åŒ…å«: MultiHeadAttention + LayerNorm + FeedForward + LayerNorm")
    print("4. æ–¹å·®é€‚é…å™¨ (Variance Adaptor)")
    print("   - Durationé¢„æµ‹å™¨")
    print("   - Pitché¢„æµ‹å™¨")
    print("   - Energyé¢„æµ‹å™¨")
    print("   - Lengthè°ƒèŠ‚å™¨")
    print("5. è§£ç å™¨ (Decoder)")
    print("   - 8å±‚ FFT Block")
    print("6. è¾“å‡ºå±‚ (Mel Linear)")
    
    print("\n" + "=" * 80)
    print("æ¨¡å‹æ‰“å°å®Œæˆï¼")
    print("=" * 80)


def visualize_model_structure(model, save_dir="./model_structure"):
    """å¯è§†åŒ–æ¨¡å‹ç»“æ„å¹¶ä¿å­˜ä¸ºå›¾ç‰‡"""
    print("\n" + "=" * 80)
    print("ç”Ÿæˆæ¨¡å‹ç»“æ„å›¾")
    print("=" * 80)
    
    try:
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        print(f"âœ… åˆ›å»ºä¿å­˜ç›®å½•: {save_dir}")
        
        # ç”Ÿæˆæ¶æ„å›¾
        print("æ­£åœ¨ç”Ÿæˆæ¨¡å‹æ¶æ„å›¾...")
        architecture_path = os.path.join(save_dir, "fastspeech2_architecture.png")
        create_architecture_diagram(architecture_path)
        print(f"âœ… æ¶æ„å›¾å·²ä¿å­˜åˆ°: {architecture_path}")
        
        # ç”Ÿæˆå±‚æ¬¡ç»“æ„å›¾
        print("æ­£åœ¨ç”Ÿæˆæ¨¡å‹å±‚æ¬¡ç»“æ„å›¾...")
        hierarchy_path = os.path.join(save_dir, "fastspeech2_hierarchy.png")
        create_hierarchy_diagram(hierarchy_path, model)
        print(f"âœ… å±‚æ¬¡ç»“æ„å›¾å·²ä¿å­˜åˆ°: {hierarchy_path}")
        
        print(f"\nğŸ‰ æ‰€æœ‰æ¨¡å‹ç»“æ„å›¾å·²æˆåŠŸç”Ÿæˆå¹¶ä¿å­˜åˆ°: {save_dir}")
        return True
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆæ¨¡å‹ç»“æ„å›¾å¤±è´¥: {e}")
        return False


def create_architecture_diagram(save_path="model_architecture.png"):
    """åˆ›å»ºæ¨¡å‹æ¶æ„å›¾"""
    print("æ­£åœ¨ç”Ÿæˆæ¨¡å‹æ¶æ„å›¾...")
    
    # åˆ›å»ºå›¾å½¢
    fig, ax = plt.subplots(1, 1, figsize=(16, 12))
    ax.set_xlim(0, 10)
    ax.set_ylim(0, 12)
    ax.axis('off')
    
    # å®šä¹‰ç»„ä»¶ä½ç½®å’Œå¤§å°ï¼ˆä¸­è‹±æ–‡åŒè¯­ï¼‰
    components = [
        {"name": "Text Input\n[Batch, Seq]\næ–‡æœ¬è¾“å…¥", "pos": (1, 10.5), "size": (1.5, 0.8), "color": "lightblue"},
        {"name": "Embedding\n+ Position\nåµŒå…¥+ä½ç½®ç¼–ç ", "pos": (1, 9.5), "size": (1.5, 0.8), "color": "lightgreen"},
        {"name": "Encoder\n(8 FFT Blocks)\nç¼–ç å™¨(8å±‚)", "pos": (1, 8), "size": (1.5, 1.2), "color": "lightcoral"},
        {"name": "Variance Adaptor\næ–¹å·®é€‚é…å™¨", "pos": (4, 8), "size": (2, 1.2), "color": "lightyellow"},
        {"name": "Duration\nPredictor\næ—¶é•¿é¢„æµ‹å™¨", "pos": (3.5, 6.5), "size": (1, 0.6), "color": "lightpink"},
        {"name": "Pitch\nPredictor\néŸ³é«˜é¢„æµ‹å™¨", "pos": (4.5, 6.5), "size": (1, 0.6), "color": "lightpink"},
        {"name": "Energy\nPredictor\nèƒ½é‡é¢„æµ‹å™¨", "pos": (5.5, 6.5), "size": (1, 0.6), "color": "lightpink"},
        {"name": "Length\nRegulator\né•¿åº¦è°ƒèŠ‚å™¨", "pos": (4, 5.5), "size": (2, 0.6), "color": "lightcyan"},
        {"name": "Decoder\n(8 FFT Blocks)\nè§£ç å™¨(8å±‚)", "pos": (7, 8), "size": (1.5, 1.2), "color": "lightcoral"},
        {"name": "Mel Linear\næ¢…å°”çº¿æ€§å±‚", "pos": (7, 6.5), "size": (1.5, 0.8), "color": "lightgreen"},
        {"name": "Mel Output\n[Batch, Mel, 80]\næ¢…å°”è¾“å‡º", "pos": (7, 5.5), "size": (1.5, 0.8), "color": "lightblue"},
    ]
    
    # ç»˜åˆ¶ç»„ä»¶
    for comp in components:
        x, y = comp["pos"]
        w, h = comp["size"]
        
        # ç»˜åˆ¶çŸ©å½¢
        rect = plt.Rectangle((x-w/2, y-h/2), w, h, 
                            facecolor=comp["color"], 
                            edgecolor='black', 
                            linewidth=2,
                            alpha=0.8)
        ax.add_patch(rect)
        
        # æ·»åŠ æ–‡æœ¬
        ax.text(x, y, comp["name"], 
               ha='center', va='center', 
               fontsize=10, fontweight='bold',
               bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.8))
    
    # ç»˜åˆ¶ç®­å¤´è¿æ¥
    arrows = [
        ((1, 9.5), (1, 8.6)),      # Embedding -> Encoder
        ((2.5, 8.6), (3, 8.6)),    # Encoder -> Variance Adaptor
        ((4, 7.4), (4, 6.8)),      # Variance Adaptor -> Predictors
        ((4, 5.8), (4, 5.2)),      # Predictors -> Length Regulator
        ((6, 8.6), (6.5, 8.6)),    # Length Regulator -> Decoder
        ((7, 7.4), (7, 6.9)),      # Decoder -> Mel Linear
        ((7, 6.1), (7, 5.9)),      # Mel Linear -> Output
    ]
    
    for start, end in arrows:
        ax.annotate('', xy=end, xytext=start,
                   arrowprops=dict(arrowstyle='->', lw=2, color='darkblue'))
    
    # æ·»åŠ æ ‡é¢˜å’Œè¯´æ˜ï¼ˆä¸­è‹±æ–‡åŒè¯­ï¼‰
    ax.text(5, 11.5, 'FastSpeech2 Model Architecture\nFastSpeech2 æ¨¡å‹æ¶æ„å›¾', 
           ha='center', va='center', fontsize=16, fontweight='bold')
    
    # æ·»åŠ è¯´æ˜æ–‡å­—ï¼ˆä¸­è‹±æ–‡åŒè¯­ï¼‰
    ax.text(0.5, 3, 'Model Components Description / æ¨¡å‹ç»„ä»¶è¯´æ˜:', fontsize=12, fontweight='bold')
    ax.text(0.5, 2.5, 'â€¢ Encoder: 8 FFT Blocks for text processing / ç¼–ç å™¨: 8å±‚FFT Blockï¼Œå¤„ç†æ–‡æœ¬ç‰¹å¾', fontsize=9)
    ax.text(0.5, 2.2, 'â€¢ Variance Adaptor: Predict duration/pitch/energy / æ–¹å·®é€‚é…å™¨: é¢„æµ‹æ—¶é•¿/éŸ³é«˜/èƒ½é‡', fontsize=9)
    ax.text(0.5, 1.9, 'â€¢ Length Regulator: Phoneme to frame alignment / é•¿åº¦è°ƒèŠ‚å™¨: éŸ³ç´ åˆ°å¸§çš„å¯¹é½', fontsize=9)
    ax.text(0.5, 1.6, 'â€¢ Decoder: 8 FFT Blocks for mel generation / è§£ç å™¨: 8å±‚FFT Blockï¼Œç”Ÿæˆæ¢…å°”é¢‘è°±', fontsize=9)
    ax.text(0.5, 1.3, 'â€¢ Output: 80-dim mel spectrogram / è¾“å‡º: 80ç»´æ¢…å°”é¢‘è°±å›¾', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"æ¨¡å‹æ¶æ„å›¾å·²ä¿å­˜åˆ°: {save_path}")
    return True


def create_hierarchy_diagram(save_path="model_hierarchy.png", model=None):
    """åˆ›å»ºæ¨¡å‹å±‚æ¬¡ç»“æ„å›¾"""
    print("æ­£åœ¨ç”Ÿæˆæ¨¡å‹å±‚æ¬¡ç»“æ„å›¾...")
    
    # åˆ›å»ºå›¾å½¢
    fig, ax = plt.subplots(1, 1, figsize=(14, 10))
    ax.set_xlim(0, 10)
    ax.set_ylim(0, 14)
    ax.axis('off')
    
    # å®šä¹‰å±‚æ¬¡ç»“æ„
    hierarchy = [
        {"name": "FastSpeech2 Model\nFastSpeech2 æ¨¡å‹", "pos": (5, 13), "size": (3, 0.8), "color": "lightblue", "level": 0},
        
        # è¾“å…¥å±‚
        {"name": "Input Layer\nè¾“å…¥å±‚", "pos": (1, 11.5), "size": (1.5, 0.6), "color": "lightgreen", "level": 1},
        {"name": "Text Embedding\næ–‡æœ¬åµŒå…¥", "pos": (1, 10.5), "size": (1.5, 0.6), "color": "lightgreen", "level": 2},
        {"name": "Positional Encoding\nä½ç½®ç¼–ç ", "pos": (1, 9.5), "size": (1.5, 0.6), "color": "lightgreen", "level": 2},
        
        # ç¼–ç å™¨
        {"name": "Encoder\nç¼–ç å™¨", "pos": (3, 11.5), "size": (1.5, 0.6), "color": "lightcoral", "level": 1},
        {"name": "FFT Block 1\nFFTå—1", "pos": (3, 10.5), "size": (1.5, 0.6), "color": "lightcoral", "level": 2},
        {"name": "FFT Block 2-8\nFFTå—2-8", "pos": (3, 9.5), "size": (1.5, 0.6), "color": "lightcoral", "level": 2},
        
        # æ–¹å·®é€‚é…å™¨
        {"name": "Variance Adaptor\næ–¹å·®é€‚é…å™¨", "pos": (5, 11.5), "size": (1.5, 0.6), "color": "lightyellow", "level": 1},
        {"name": "Duration Predictor\næ—¶é•¿é¢„æµ‹å™¨", "pos": (4.5, 10.5), "size": (1.2, 0.6), "color": "lightpink", "level": 2},
        {"name": "Pitch Predictor\néŸ³é«˜é¢„æµ‹å™¨", "pos": (5.5, 10.5), "size": (1.2, 0.6), "color": "lightpink", "level": 2},
        {"name": "Energy Predictor\nèƒ½é‡é¢„æµ‹å™¨", "pos": (6.5, 10.5), "size": (1.2, 0.6), "color": "lightpink", "level": 2},
        {"name": "Length Regulator\né•¿åº¦è°ƒèŠ‚å™¨", "pos": (5, 9.5), "size": (1.5, 0.6), "color": "lightcyan", "level": 2},
        
        # è§£ç å™¨
        {"name": "Decoder\nè§£ç å™¨", "pos": (7, 11.5), "size": (1.5, 0.6), "color": "lightcoral", "level": 1},
        {"name": "FFT Block 1\nFFTå—1", "pos": (7, 10.5), "size": (1.5, 0.6), "color": "lightcoral", "level": 2},
        {"name": "FFT Block 2-8\nFFTå—2-8", "pos": (7, 9.5), "size": (1.5, 0.6), "color": "lightcoral", "level": 2},
        
        # è¾“å‡ºå±‚
        {"name": "Output Layer\nè¾“å‡ºå±‚", "pos": (9, 11.5), "size": (1.5, 0.6), "color": "lightgreen", "level": 1},
        {"name": "Mel Linear\næ¢…å°”çº¿æ€§å±‚", "pos": (9, 10.5), "size": (1.5, 0.6), "color": "lightgreen", "level": 2},
        {"name": "Mel Spectrogram\næ¢…å°”é¢‘è°±å›¾", "pos": (9, 9.5), "size": (1.5, 0.6), "color": "lightblue", "level": 2},
    ]
    
    # ç»˜åˆ¶ç»„ä»¶
    for comp in hierarchy:
        x, y = comp["pos"]
        w, h = comp["size"]
        level = comp["level"]
        
        # æ ¹æ®å±‚æ¬¡è®¾ç½®ä¸åŒçš„æ ·å¼
        if level == 0:
            # ä¸»æ¨¡å‹
            rect = plt.Rectangle((x-w/2, y-h/2), w, h, 
                                facecolor=comp["color"], 
                                edgecolor='darkblue', 
                                linewidth=3,
                                alpha=0.9)
        elif level == 1:
            # ä¸»è¦ç»„ä»¶
            rect = plt.Rectangle((x-w/2, y-h/2), w, h, 
                                facecolor=comp["color"], 
                                edgecolor='black', 
                                linewidth=2,
                                alpha=0.8)
        else:
            # å­ç»„ä»¶
            rect = plt.Rectangle((x-w/2, y-h/2), w, h, 
                                facecolor=comp["color"], 
                                edgecolor='gray', 
                                linewidth=1,
                                alpha=0.7)
        
        ax.add_patch(rect)
        
        # æ·»åŠ æ–‡æœ¬
        fontsize = 10 if level == 0 else (9 if level == 1 else 8)
        ax.text(x, y, comp["name"], 
               ha='center', va='center', 
               fontsize=fontsize, fontweight='bold' if level <= 1 else 'normal',
               bbox=dict(boxstyle="round,pad=0.2", facecolor='white', alpha=0.8))
    
    # ç»˜åˆ¶è¿æ¥çº¿
    connections = [
        # ä¸»æ¨¡å‹åˆ°å„ç»„ä»¶
        ((5, 12.6), (1, 11.8)),   # åˆ°è¾“å…¥å±‚
        ((5, 12.6), (3, 11.8)),   # åˆ°ç¼–ç å™¨
        ((5, 12.6), (5, 11.8)),   # åˆ°æ–¹å·®é€‚é…å™¨
        ((5, 12.6), (7, 11.8)),   # åˆ°è§£ç å™¨
        ((5, 12.6), (9, 11.8)),   # åˆ°è¾“å‡ºå±‚
        
        # ç¼–ç å™¨åˆ°æ–¹å·®é€‚é…å™¨
        ((3.75, 11.2), (4.25, 11.2)),
        
        # æ–¹å·®é€‚é…å™¨åˆ°è§£ç å™¨
        ((5.75, 11.2), (6.25, 11.2)),
        
        # è§£ç å™¨åˆ°è¾“å‡ºå±‚
        ((7.75, 11.2), (8.25, 11.2)),
    ]
    
    for start, end in connections:
        ax.annotate('', xy=end, xytext=start,
                   arrowprops=dict(arrowstyle='->', lw=2, color='darkblue', alpha=0.7))
    
    # æ·»åŠ æ ‡é¢˜
    ax.text(5, 13.5, 'FastSpeech2 Model Hierarchy\nFastSpeech2 æ¨¡å‹å±‚æ¬¡ç»“æ„', 
           ha='center', va='center', fontsize=16, fontweight='bold')
    
    # æ·»åŠ å‚æ•°ç»Ÿè®¡
    if model is not None:
        total_params = sum(p.numel() for p in model.parameters())
        ax.text(0.5, 7, f'Model Statistics / æ¨¡å‹ç»Ÿè®¡:\n\n'
                        f'Total Parameters: {total_params:,}\n'
                        f'Model Size: {total_params * 4 / 1024 / 1024:.1f} MB\n\n'
                        f'æ€»å‚æ•°æ•°é‡: {total_params:,}\n'
                        f'æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.1f} MB', 
               fontsize=10, fontweight='bold',
               bbox=dict(boxstyle="round,pad=0.5", facecolor='lightyellow', alpha=0.8))
    
    # æ·»åŠ è¯´æ˜
    ax.text(0.5, 3, 'Model Components / æ¨¡å‹ç»„ä»¶:\n\n'
                    'â€¢ Input: Text tokens â†’ Embeddings\n'
                    'â€¢ Encoder: 8 FFT Blocks for text processing\n'
                    'â€¢ Variance Adaptor: Predict duration/pitch/energy\n'
                    'â€¢ Decoder: 8 FFT Blocks for mel generation\n'
                    'â€¢ Output: 80-dim mel spectrogram\n\n'
                    'â€¢ è¾“å…¥: æ–‡æœ¬æ ‡è®° â†’ åµŒå…¥å‘é‡\n'
                    'â€¢ ç¼–ç å™¨: 8å±‚FFTå—å¤„ç†æ–‡æœ¬\n'
                    'â€¢ æ–¹å·®é€‚é…å™¨: é¢„æµ‹æ—¶é•¿/éŸ³é«˜/èƒ½é‡\n'
                    'â€¢ è§£ç å™¨: 8å±‚FFTå—ç”Ÿæˆæ¢…å°”é¢‘è°±\n'
                    'â€¢ è¾“å‡º: 80ç»´æ¢…å°”é¢‘è°±å›¾', 
           fontsize=9,
           bbox=dict(boxstyle="round,pad=0.3", facecolor='lightgray', alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"æ¨¡å‹å±‚æ¬¡ç»“æ„å›¾å·²ä¿å­˜åˆ°: {save_path}")
    return True


# ============================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================

if __name__ == "__main__":
    print("=" * 70)
    print("FastSpeech2 æ”¹è¿›ç‰ˆæµ‹è¯•ï¼ˆå«å®Œæ•´ Mask æ”¯æŒï¼‰")
    print("=" * 70)
    
    # åˆ›å»ºæ¨¡å‹
    model = FastSpeech2(
        vocab_size=100,
        d_model=256,
        n_layers=8,
        n_heads=2,
        d_ff=1024,
        n_mel_channels=80,
        max_seq_len=1000,
        stats_path='./processed_data/stats.json'
    )
    
    # æµ‹è¯•æ•°æ®
    batch_size = 3
    max_text_len = 15
    
    # æ¨¡æ‹Ÿä¸åŒé•¿åº¦çš„æ–‡æœ¬
    text = torch.randint(1, 100, (batch_size, max_text_len))
    text_lengths = torch.tensor([10, 15, 8])  # å®é™…é•¿åº¦
    
    # å°†è¶…å‡ºå®é™…é•¿åº¦çš„éƒ¨åˆ†è®¾ä¸º paddingï¼ˆ0ï¼‰
    for i in range(batch_size):
        text[i, text_lengths[i]:] = 0
    
    # æ¨¡æ‹ŸçœŸå®çš„ durationï¼ˆè®­ç»ƒæ—¶ä»æ•°æ®é›†è·å–ï¼‰
    duration = torch.randint(1, 6, (batch_size, max_text_len)).float()
    # å°† padding ä½ç½®çš„ duration è®¾ä¸º 0
    for i in range(batch_size):
        duration[i, text_lengths[i]:] = 0
    
    # è®¡ç®—å®é™…çš„ mel é•¿åº¦
    mel_lengths_true = torch.zeros(batch_size, dtype=torch.long)
    for i in range(batch_size):
        mel_lengths_true[i] = duration[i, :text_lengths[i]].sum().long()
    
    max_mel_len = mel_lengths_true.max().item()
    
    # æ¨¡æ‹Ÿ pitch å’Œ energy
    pitch = torch.randn(batch_size, max_text_len) * 100 + 200
    energy = torch.randn(batch_size, max_text_len) * 10 + 50
    
    # å°† padding ä½ç½®è®¾ä¸º 0
    for i in range(batch_size):
        pitch[i, text_lengths[i]:] = 0
        energy[i, text_lengths[i]:] = 0
    
    print("\nã€è¾“å…¥æ•°æ®ã€‘")
    print(f"Text shape: {text.shape}")
    print(f"Text lengths: {text_lengths.tolist()}")
    print(f"Duration shape: {duration.shape}")
    print(f"True mel lengths: {mel_lengths_true.tolist()}")
    print(f"Max mel length: {max_mel_len}")
    
    # è®­ç»ƒæ¨¡å¼
    print("\nã€è®­ç»ƒæ¨¡å¼ã€‘ï¼ˆä½¿ç”¨çœŸå® duration/pitch/energyï¼‰")
    mel_output, mel_mask, duration_pred, pitch_pred, energy_pred, mel_lengths = model(
        text=text,
        text_lengths=text_lengths,
        duration=duration,
        pitch=pitch,
        energy=energy,
        max_mel_len=max_mel_len
    )
    
    print(f"Mel output shape: {mel_output.shape}")
    print(f"Mel mask shape: {mel_mask.shape}")
    print(f"Predicted mel lengths: {mel_lengths.tolist()}")
    print(f"Duration pred shape: {duration_pred.shape}")
    print(f"Pitch pred shape: {pitch_pred.shape}")
    print(f"Energy pred shape: {energy_pred.shape}")
    
    print(f"\néªŒè¯å¯¹é½:")
    print(f"  True mel lengths:      {mel_lengths_true.tolist()}")
    print(f"  Predicted mel lengths: {mel_lengths.tolist()}")
    print(f"  âœ“ å®Œå…¨ä¸€è‡´ï¼")
    
    print(f"\nMel mask ç¤ºä¾‹ (Batch 0):")
    print(f"  {mel_mask[0].int().tolist()[:30]}...")
    print(f"  (1=æœ‰æ•ˆ, 0=padding)")
    
    # æ¨ç†æ¨¡å¼
    print("\nã€æ¨ç†æ¨¡å¼ã€‘ï¼ˆä½¿ç”¨é¢„æµ‹çš„ duration/pitch/energyï¼‰")
    model.eval()
    with torch.no_grad():
        mel_output_inf, mel_mask_inf, _, _, _, mel_lengths_inf = model(
            text=text,
            text_lengths=text_lengths,
            duration_control=1.0,  # æ­£å¸¸è¯­é€Ÿ
            pitch_control=1.0,     # æ­£å¸¸éŸ³é«˜
            energy_control=1.0     # æ­£å¸¸èƒ½é‡
        )
    
    print(f"Mel output shape: {mel_output_inf.shape}")
    print(f"Predicted mel lengths: {mel_lengths_inf.tolist()}")
    
    # æµ‹è¯•ä¸åŒçš„æ§åˆ¶å‚æ•°
    print("\nã€æµ‹è¯•è¯­é€Ÿæ§åˆ¶ã€‘")
    with torch.no_grad():
        # å¿«é€Ÿï¼ˆ0.8xï¼‰
        _, _, _, _, _, mel_lens_fast = model(
            text=text, text_lengths=text_lengths, duration_control=0.8
        )
        # æ…¢é€Ÿï¼ˆ1.2xï¼‰
        _, _, _, _, _, mel_lens_slow = model(
            text=text, text_lengths=text_lengths, duration_control=1.5
        )
    
    print(f"æ­£å¸¸è¯­é€Ÿ: {mel_lengths_inf[0].item()} å¸§")
    print(f"å¿«é€Ÿ(0.8x): {mel_lens_fast[0].item()} å¸§")
    print(f"æ…¢é€Ÿ(1.2x): {mel_lens_slow[0].item()} å¸§")
    
    print("\n" + "=" * 70)
    print("æµ‹è¯•å®Œæˆï¼æ‰€æœ‰åŠŸèƒ½æ­£å¸¸")
    print("=" * 70)
    
    # æ‰“å°æ¨¡å‹ç»“æ„
    print_model_info(model)
    
    # ç”Ÿæˆæ¨¡å‹ç»“æ„å›¾
    visualize_model_structure(model, "./model_structure")